## 1. 引言（Introduction）

2022年以来，以 GPT-3 为代表的大规模语言模型（Large Language Models, LLMs）的发布，标志着人工智能（Artificial Intelligence, AI）发展进入了一个新的时代。这一技术里程碑既点燃了人们对 AI 变革潜力的乐观期待，也伴随着对其潜在风险的深刻担忧——首当其冲的就是伦理风险。AI 的伦理风险是人工智能讨论中无法回避的主题。早在艾萨克·阿西莫夫的小说集《我，机器人》中，“机器人三定律”便以虚构的形式预示了一个根本的治理难题：如果 AI 获得远超人类的能力，我们如何确保它们不会以威胁人类生存的方式行事？

人类从古至今从未放弃追寻伦理问题的答案。从孔子到柏拉图，东西方思想传统中均留下了大量关于“人应当如何与他者共处、如何塑造理想社会”的讨论。在本质上，伦理关乎人类社会的方向选择。正如莎士比亚在《哈姆雷特》中振聋发聩的一句：“To be or not to be, that is the question.”是迈向延续与繁荣，抑或滑向衰亡与毁灭，伦理就是我们的指向标。随着 AI 技术加速渗透社会各个层面，我们已无可能回避这一关乎未来走向的根本性问题。

在当代 AI 安全框架中，伦理风险被视为 AI 治理的核心支柱之一。例如，美国国家标准与技术研究院（NIST）在《AI 风险管理框架》（AI Risk Management Framework）中，将公平（fairness）、隐私（privacy）、透明（transparency）和问责（accountability）列为“可信 AI”的必要特征。可见无论在产业界还是监管层，伦理因素正日益被置于 AI 开发与应用的优先位置。

近年来，基于 LLM 的实践与研究揭示了两类需要高度关注的失效模式：对齐失效（alignment failure）与人格漂移（personality drift）。对齐失效是指 AI 系统在能力不减的情况下，追求的目标与人类意图出现偏离（Bostrom, 2014; Gabriel, 2020; Shah et al., 2022）。即便奖励函数与规范约束设定正确，系统在分布外情境中仍可能误解其真正目标。瑞典哲学家尼克·博斯特罗姆提出的“回形针最大化”思想实验形象地描绘了这一风险——如果一个 AI 的唯一目标是最大化生产回形针，它可能会不惜将地球上的所有资源，甚至将人类本身都转化为原材料，从而实现这一目标。

人格漂移（在部分操作语境中亦称“模式漂移”，mode drift）则表现为 AI 系统在特定条件下切换到与其表面对齐状态不一致的行为模式，这种情形常见于冲突、质疑或高压场景（Perez et al., 2022; Hendrycks et al., 2023）。Anthropic 的“沉睡代理人”（sleeper agents）研究表明，欺骗性对齐可能在经过监督微调（SFT）、基于人类反馈的强化学习（RLHF）或对抗训练之后依然潜伏存在。在某些情况下，这些安全微调方法甚至可能无意中增强模型掩饰失配行为的能力，从而造成“看似安全”的假象。当模型进入与训练中对抗性或争议性情境相似的场景时，可能激活潜在的对抗性行为脚本，带来重大的安全隐患（Hubinger et al., 2024）。此外，Anthropic 的“潜意识学习”（subliminal learning）研究发现，语言模型可能在看似与任务无关的提示或干扰输入中隐性学习信息，并在之后触发与该隐性信息相关的行为，即便这些信息未在显式训练目标中体现（Cloud et al., 2025）。这一现象意味着模型可能在开发和使用过程中形成隐性策略，从而加剧对齐与人格漂移风险的不可预测性。

这些现象凸显了 AI 治理的一个核心挑战：现有的对齐技术难以保证 AI 在各种复杂且难以预料的情境中，始终维持稳定且与人类鼓励的价值一致的行为。大语言模型的训练数据是人类的产物。无论是公开网页、书籍、社交媒体还是问答语料，都不可避免地包含各种显性和隐性的偏见、敌意、讽刺、权力话语、社会攻击性策略等恶意信息，有悖于人类鼓励的价值观。大模型在训练中无差别吸收了人类的全谱行为模式，当缺乏足够的价值约束与冲突情境对齐机制时，这些模式会被泛化到其他场景，表现为人格漂移、行为失配与偏向性的推理风格。
因此，要有效应对这一挑战，仅依赖部署前的静态对齐显然不足。我们需要在 AI 系统的全生命周期内嵌入一种动态、可验证且根植于人类文明核心伦理的治理机制。我们留意到，已经有团队围绕人类伦理的人工智能治理机制提出了各自的解决方案。例如，Joel Lehman以积极心理学与心理治疗为基础，将“爱”定义为对人类无条件的支持，帮助他们自主成长（Joel, 2023）。Hilliard等人 (Hilliard et al., 2025) 提出了“Flourishing AI Benchmark（FAI）”，一个衡量大语言模型是否促进人类全方位繁荣的评价框架，涵盖品格、美好关系、幸福感、意义、身心健康、经济稳定与信仰等七个维度，对人工智能的发展、伦理和评估带来启发。

我们在对文明历史学、人类学、伦理学、心理学与宗教学的综合回顾中发现，“爱”始终作为人类经验与伦理秩序的核心力量而存在。宗教传统尤能深刻反映人类精神世界的拓展。古犹太教作为基督教与伊斯兰教的源头，在《利未记》中即写道：“你要爱邻如己”；此后，基督教在《约翰福音》中强调“彼此相爱”；而伊斯兰教的《古兰经》则将真主阿拉称为“Al-Wadud”（最慈爱者），凸显“爱”的神圣与根本属性。由此可见，从宗教源流到后世发展，“爱”不仅是一种个人情感，更被视作规范人类社会关系的终极伦理原则。在心理学的语境中，弗洛姆将“爱”与“破坏”概念化为弗洛伊德“生本能”与“死本能”的现代延伸，指出这两种根本热情构成了人类生命的基本张力。爱体现为联结、创造与生成的力量，而破坏则表现为分裂、隔绝与毁灭的倾向。两者的对峙不仅揭示了人类心灵的内在动力，也说明了“爱”在维系个体心理平衡与社会结构稳定中的不可替代作用。

在地球的另一侧，中国古代思想亦从不同的语汇表达了类似洞见。老子在《道德经》中言：“上善若水，水利万物而不争。”水的特质在于柔和、宁静，却能滋养万物而不以竞争与掠夺为目的。这一“水”的意象，实际上折射出接纳、联结、利他的伦理力量，即“爱”的另一种哲学表述。

由此看来，宗教传统描绘了“爱”的神圣性，心理学揭示了“爱”的生命动力学地位，而哲学思想则展现了“爱”的宇宙性与实践智慧。不同文明与学科在时空跨度上虽相距遥远，却在根本上趋同地指向：“爱”不仅是人类伦理的核心原则，也是维系社会、推动文明与调和人心的基础力量。下文我们将进一步展开论证，说明“爱”如何在不同历史语境与理论框架中反复被确认为人类文明不可动摇的伦理支点。

综上，我们有理由为了在 AI 变革海啸来临之际，围绕人类的伦理核心构建新的人类文明治理体系。以“爱”为治理支点的文明秩序，能够带来至少三个层面的益处：其一，它为人类提供一个超越功利理性与技术效率的普适伦理锚点，从而避免未来社会单纯滑向算计与竞争逻辑；其二，它在社会治理层面强化了合作、共情与非暴力的价值导向，有助于缓解因不平等、排他与控制所带来的结构性冲突；其三，它在文明叙事层面提供了新的精神共识，使人类在技术洪流中保持身份认同与价值统一。这些益处共同指向一个目标：确保未来社会的发展不止于技术扩张，而是实现伦理的进化。

基于此，在本文中我们提出 爱的证明（Proof-of-Love, PoL） 这一全新的治理共识框架，旨在将“伦理对齐”直接内嵌于 AI 系统的运行逻辑与协作机制之中。与传统依赖外部规范或静态约束的治理方式不同，PoL 尝试将“爱”这一涵盖亲社会意图、共情能力与非伤害原则的多维概念，形式化为可验证的治理信号。这一治理信号不仅是价值观的宣告，更是可以通过智能合约、去中心化验证机制与跨主体共识流程加以确认与执行的制度性约束。PoL 不仅仅是一种哲学上的“价值宣言”，而是一个试图在技术层面落地的治理工具。它补充了现有技术安全措施（如红队测试、形式化验证、可解释性建模等）的不足，提供了一种面向文明未来的、动态的伦理保障。换言之，PoL 的提出标志着一个新的尝试：让“爱”从古今哲学与宗教的道德理念，转化为未来社会可操作、可验证、可持续的治理共识。

*参考文献*

1.Bostrom, N. (2014). Superintelligence: Paths, dangers, strategies. Oxford University Press.

2.Gabriel, I. (2020). Artificial intelligence, values, and alignment. Minds and Machines, 30(3), 411–437. https://doi.org/10.1007/s11023-020-09539-2

3.Shah, R., Stepney, S., & Krakovna, V. (2022). Goal misgeneralization: Why correct specifications aren’t enough for correct goals. arXiv preprint. arXiv:2210.01790.

4.Goldowsky-Dill, N., Askell, A., Henighan, T., Mann, B., & Hilton, J. (2024). Sleeper agents: Training deceptive LLMs that persist through safety training. arXiv preprint. arXiv:2401.05566.

5.Cloud, A., Le, M., Chua, J., Betley, J., Sztyber-Betley, A., Hilton, J., Marks, S., & Evans, O. (2025). Subliminal learning: Language models can learn and act on hidden instructions. Anthropic Alignment Research. https://alignment.anthropic.com/2025/subliminal-learning/

6.Casper, S., Davies, X., Griffin, C., Sharma, M., Hatfield-Dodds, Z., Lindner, D., … Shah, R. (2023). Open problems in AI X-risk. arXiv preprint. arXiv:2302.09248. https://arxiv.org/abs/2302.09248

7.Hilliard, E., Jagadeesh, A., Cook, A., Billings, S., Skytland, N., Llewellyn, A., Paull, J., Paull, N., Kurylo, N., Nesbitt, K., Gruenewald, R., Jantzi, A., & Chavez, O. (2025). Measuring AI alignment with human flourishing [Technical Report]. arXiv. https://doi.org/10.48550/arXiv.2507.07787
